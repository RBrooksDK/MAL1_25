<h1 align="center">07 Logistic Regression and Gradient Descend</h1>

## Material:
Section about "Logistic Regression" in Ch 4, pp. 136 - 146 <!-- NOTE: probably something more than that -->

[Session material](https://viaucdk-my.sharepoint.com/:f:/g/personal/rib_viauc_dk/Ehw3HAQLiH5OuSB14MmZb5gBZxQvOeMDPqkRz7Jvg2hBjw?e=ADVcEv)

## Topics

This lecture will cover the logistic regression algorithm <!-- NOTE: should be specified -->

After attending this lecture and reading the corresponding part of the book, I expect you to be able to:

- Train a logistic regression (LR) and k Nearest Neighbors (kNN) algorithm on a dataset in sklearn
- Explain the principles behind the kNN algorithm
- Explain the key ideas behind logistic regression, and implement a logistic regression classifier in python.
- Explain the concept of “maximum likelihood” and how it is used.
- Explain and use L1 and L2 regularization in the context of logistic regression, and discuss the difference between these approaches, as well as the importance of the hyperparameter C.
- Discuss advantages and disadvantages of logistic regression.
- Explain what is meant by the "hyperparameters" of an algorithm
<!-- NOTE: I wasn't sure what exactly to take from MAL 01 -->

