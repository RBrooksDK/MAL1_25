{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Machine Learning Course - Spring 2025 <p>Repository for MAL1-S25 at VIA</p> <p>Checkout the homepage!</p> </p> <p>Jump to Lesson Plan</p> <p>Jump to Portfolios and Project</p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>Bachelor level course - the course is academically challenging working on problems independently.</li> <li>Grade: 7-step scale</li> <li>Recommended prerequisites: In \"Sessions\" in the left menu, a dedicated entry is made for prerequisites.</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended to work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#literature-resources","title":"Literature &amp; Resources","text":"<p>G\u00e9ron, Aur\u00e9lien: Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow, 3<sup>rd</sup> Edition. (the second edition will also do)</p> <p>We highly recommend retrieving a copy of the book \u2013 it will also be the course book for the Deep Learning course in the Autumn.</p>"},{"location":"#software","title":"Software","text":"<p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. The easiest way to install Python and Jupyter is using Anaconda Distribution. You can choose whichever framework you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p> <p>The course will be somewhat \"Python-heavy\" and during the course, it is expected that you can solve relatively complex machine learning problems in Python (in your assignments and project). It is expected that you are able to work in Python or learn to do so relatively fast.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>There are three main areas that are important when it comes to Machine Learning</p> <ul> <li>Programming</li> <li>Linear Algebra</li> <li>Probability theory and statistics.</li> </ul> <p>We assume that you have (1) covered! Linear Algebra knowledge you can obtain either by following our course (IT-ALI1) or you can see some suggestions below under Online Resource. The same goes for item (3). Now, it is possible to master machine learning without knowing anything about linear algebra or probability theory, but some topics will most likely be easier to comprehend if you have some background knowledge about the underlying mathematical foundation.</p>"},{"location":"#online-resources","title":"Online Resources","text":"<p>There are several Python-programming tutorials on YouTube, also ones that are data science / Machine Learning oriented. I recommend Alexander Ihler\u2019s course Machine Learning and Data Mining.</p> <p>For prerequisites, we have our own course here at VIA called Applied Linear Algebra. You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2023).</p> <p>In terms of probability theory and statistics, we have our own course here at VIA called Stochastic Modelling and Processing (IT-SMP1). You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2021). </p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Introduction to Machine Learning was first offered in the spring of 2018 and has been scheduled 1-2 times per year since then. The course responsible is Richard Brooks (RIB). </p> <p>Grade Distribution 2023 (ordinary exam only)</p> Grade Count 12 13 10 7 7 15 4 7 02 5 00 6"},{"location":"07_Logistic_Regression_and_Gradient_Descent/","title":"07 Logistic Regression and Gradient Descent","text":"07 Logistic Regression and Gradient Descend"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#material","title":"Material:","text":"<p>Section about \"Logistic Regression\" in Ch 4, pp. 136 - 146 </p> <p>Session material</p>"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#topics","title":"Topics","text":"<p>This lecture will cover the logistic regression algorithm </p> <p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Train a logistic regression (LR) model on a dataset.</li> <li>Understand the difference between linear regression and logistic regression.</li> <li>Understand the concept of maximum likelihood estimation.</li> <li>Explain the key ideas behind logistic regression, and implement a logistic regression classifier in python.</li> <li>Explain and use L1 and L2 regularization in the context of logistic regression, and discuss the difference between these approaches, as well as the importance of the hyperparameter C.</li> <li>Discuss advantages and disadvantages of logistic regression.</li> </ul>"},{"location":"08_Support_Vector_Machines/","title":"08 Support Vector Machines","text":"08 Support Vector Machines"},{"location":"08_Support_Vector_Machines/#preparation","title":"Preparation","text":"<p>Ch 5</p>"},{"location":"08_Support_Vector_Machines/#material","title":"Material","text":"<p>Watch the video lectures below. here is the material from the videos.</p> <p>Slides</p> <p>Code</p>"},{"location":"08_Support_Vector_Machines/#session-description","title":"Session Description","text":"<p>This lecture introduces a more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM).</p> <p>We will start with the basic concepts of SVM, including the idea of support vectors and how they influence the decision boundary. We will then discuss how to make non-linearly separable data linearly separable by adding new features. The kernel trick will be introduced as a powerful method to handle non-linear data, and we will explore how different kernels can be used in SVM.</p>"},{"location":"08_Support_Vector_Machines/#key-concepts","title":"Key Concepts","text":"<ul> <li>SVM Classification</li> <li>Support Vectors</li> <li>Decision Boundary</li> <li>Linear Separability</li> <li>Feature Transformation</li> <li>Kernel Trick</li> <li>Kernel Functions</li> <li>Non-linear Classification</li> </ul>"},{"location":"08_Support_Vector_Machines/#learning-objectives","title":"Learning Objectives","text":"<p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.</li> <li>Explain and exemplify how adding new features can make non-linearly separable data linearly separable.</li> <li>Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.</li> <li>Discuss how, when using a Gaussian kernel,\u00a0the hyperparameters C and\u00a0\u03b3 influence the decision boundaries.</li> <li>Discuss advantages and disadvantages of support vector machines.</li> <li>Discuss the concept of hyperparameters in the context of SVM, and demonstrate how to tune them to improve model performance.</li> </ul>"},{"location":"08_Support_Vector_Machines/#video-lectures","title":"Video Lectures","text":"<p>Playlist with all videos (Opens in Youtube)</p>"},{"location":"08_Support_Vector_Machines/#81-svm-introduction-to-svm-part-1","title":"8.1. SVM - Introduction to SVM (Part 1)","text":""},{"location":"08_Support_Vector_Machines/#82-svm-soft-margin-svm-part-2","title":"8.2. SVM - Soft Margin SVM (Part 2)","text":""},{"location":"08_Support_Vector_Machines/#83-svm-multiclass-classification-part-3","title":"8.3. SVM - Multiclass classification (Part 3)","text":""},{"location":"08_Support_Vector_Machines/#84-svm-non-linear-classification-part-4","title":"8.4. SVM - Non-linear classification (Part 4)","text":""},{"location":"08_Support_Vector_Machines/#85-svm-code-example-part-5","title":"8.5. SVM - Code example (Part 5)","text":""},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left to access a plan of a specific session and additional resources for that session.</p> Session Date Topic 07 25 Mar 12:45 \u2013 16:05 Logistic Regression and Gradient Descent 08 1 Apr 12:45 \u2013 16:05 Support Vector Machines 09 8 Apr 12:45 \u2013 16:05 Neural Networks 10 22 Apr 12:45 \u2013 16:05 Dimensionality Reduction 11 29 Apr 12:45 \u2013 16:05 Clustering 12 To be specified later Recap"},{"location":"blog/","title":"Blog","text":""}]}