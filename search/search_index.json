{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Machine Learning Course - Spring 2025 <p>Repository for MAL1-S25 at VIA</p> <p>Checkout the homepage!</p> </p> <p>Jump to Lesson Plan</p> <p>Jump to Portfolios and Project</p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>Bachelor level course - the course is academically challenging working on problems independently.</li> <li>Grade: 7-step scale</li> <li>Recommended prerequisites: In \"Sessions\" in the left menu, a dedicated entry is made for prerequisites.</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended to work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#literature-resources","title":"Literature &amp; Resources","text":"<p>G\u00e9ron, Aur\u00e9lien: Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow, 3<sup>rd</sup> Edition. (the second edition will also do)</p> <p>We highly recommend retrieving a copy of the book \u2013 it will also be the course book for the Deep Learning course in the Autumn.</p>"},{"location":"#software","title":"Software","text":"<p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. The easiest way to install Python and Jupyter is using Anaconda Distribution. You can choose whichever framework you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p> <p>The course will be somewhat \"Python-heavy\" and during the course, it is expected that you can solve relatively complex machine learning problems in Python (in your assignments and project). It is expected that you are able to work in Python or learn to do so relatively fast.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>There are three main areas that are important when it comes to Machine Learning</p> <ul> <li>Programming</li> <li>Linear Algebra</li> <li>Probability theory and statistics.</li> </ul> <p>We assume that you have (1) covered! Linear Algebra knowledge you can obtain either by following our course (IT-ALI1) or you can see some suggestions below under Online Resource. The same goes for item (3). Now, it is possible to master machine learning without knowing anything about linear algebra or probability theory, but some topics will most likely be easier to comprehend if you have some background knowledge about the underlying mathematical foundation.</p>"},{"location":"#online-resources","title":"Online Resources","text":"<p>There are several Python-programming tutorials on YouTube, also ones that are data science / Machine Learning oriented. I recommend Alexander Ihler\u2019s course Machine Learning and Data Mining.</p> <p>For prerequisites, we have our own course here at VIA called Applied Linear Algebra. You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2023).</p> <p>In terms of probability theory and statistics, we have our own course here at VIA called Stochastic Modelling and Processing (IT-SMP1). You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2021). </p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Introduction to Machine Learning was first offered in the spring of 2018 and has been scheduled 1-2 times per year since then. The course responsible is Richard Brooks (RIB). </p> <p>Grade Distribution 2023 (ordinary exam only)</p> Grade Count 12 13 10 7 7 15 4 7 02 5 00 6"},{"location":"01_Introduction/","title":"01 Introduction to Machine Learning","text":"01 Introduction"},{"location":"01_Introduction/#material","title":"Material","text":"<p>Chapter 1 + the pdf \"kNN\"</p> <p>kNN (PDF)</p> <p>Lesson 1 (PDF)</p> <p>Titanic (CSV)</p> <p>Titanic (Jupyter Notebook)</p>"},{"location":"01_Introduction/#session-description","title":"Session Description","text":"<p>We will talk about what machine learning is and the types of problems we work with. We will also introduce the first algorithm: k-Nearest Neighbor.</p>"},{"location":"01_Introduction/#key-concepts","title":"Key Concepts","text":"<ul> <li>Distance metrics in kNN</li> <li>The parameter \u201ck\u201d and its effect on model performance</li> <li>Differences between low bias and low variance models</li> <li>Data normalization for distance-based methods</li> <li>Evaluating model outputs (accuracy, confusion matrix, etc.)</li> </ul>"},{"location":"01_Introduction/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Explain what is meant by the term Machine Learning (ML)</li> <li>Explain what is meant by supervised vs. unsupervised learning</li> <li>Explain the overall difference between classification and regression</li> <li>Describe the \"train-test\" methodology</li> <li>Train a k-Nearest Neighbors (kNN) algorithm on a dataset in sklearn</li> <li>Explain the principles behind the kNN algorithm</li> <li>Explain what is meant by the \"hyperparameters\" of an algorithm</li> </ul>"},{"location":"02_Mathematical_Background/","title":"02 Mathematical Background","text":"02 Mathematical Background"},{"location":"02_Mathematical_Background/#preparation","title":"Preparation","text":""},{"location":"02_Mathematical_Background/#material","title":"Material","text":"<p>Session Notes</p>"},{"location":"02_Mathematical_Background/#session-description","title":"Session Description","text":"<p>In today's session we look at some of the math behind Machine Learning. Literature: The video \"Linear Algebra - Math for Machine Learning\". For a very appealing and visual explanation of SVD, you should take a look at Visual Kernel's video on the topic \"SVD Visualized\".</p>"},{"location":"02_Mathematical_Background/#key-concepts","title":"Key Concepts","text":"<ul> <li>The fundamental role of linear algebra in data representation and computations</li> <li>How GPUs harness matrix operations to accelerate machine learning</li> <li>Applying advanced techniques like Singular Value Decomposition (SVD)</li> <li>Bridging the gap between mathematical concepts and programming implementations</li> </ul>"},{"location":"02_Mathematical_Background/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand why mathematics, particularly linear algebra, calculus, and statistics, is fundamental to machine learning and its optimization processes</li> <li>Gain knowledge on how linear algebra facilitates the representation of machine learning data, models, and computations using arrays and matrices.</li> <li>Recognize the critical role of GPUs and matrix operations in enhancing the efficiency and capabilities of machine learning algorithms.</li> <li>Develop an intuitive grasp of linear algebra by relating it to programming concepts.</li> <li>Learn about the application of linear algebra techniques, such as Singular Value Decomposition, in processing and optimizing machine learning data and models.</li> </ul>"},{"location":"02_Mathematical_Background/#video-lectures","title":"Video Lectures","text":""},{"location":"03_Regression/","title":"03 Regression","text":"03 Regression"},{"location":"03_Regression/#preparation","title":"Preparation","text":"<p>Ch 4 (except \"Logistic Regression\")</p>"},{"location":"03_Regression/#material","title":"Material","text":"<p>Session material: In this folder. </p> <p>Hitters.csv</p> <p>Regression - Empty (PDF)</p> <p>Regression X (PDF)</p> <p>Regression Y (PDF)</p> <p>Regression - Hitters (Jupyter Notebook)</p>"},{"location":"03_Regression/#session-description","title":"Session Description","text":"<p>Today, we are going to look at regression algorithms, where instead of predicting a class you predict some continuous variable.</p> <p>We also discuss (again) what is meant by \u201cregularization\u201d and consider the R\u00b2 performance metric for regression. Last but not least we will introduce one of the most important concepts in Machine Learning: Gradient Descent.</p>"},{"location":"03_Regression/#key-concepts","title":"Key Concepts","text":"<ul> <li>Ordinary Least Squares (OLS) regression</li> <li>Ridge regression</li> <li>Lasso regression</li> </ul>"},{"location":"03_Regression/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Explain what is meant by \"regression\" and in which contexts to apply it</li> <li>Explain the following linear regression models, their strengths and weaknesses, and apply them in python:</li> <li>Ordinary Least Squares (OLS) regression</li> <li>Ridge regression</li> <li>Lasso regression</li> <li>Elastic Net Regression</li> <li>Explain what is meant by the term \"regularization\" in an ML-context</li> <li>Describe what is meant by \"bias\" and \"variance\" in relation to ML-algorithms</li> <li>Explain the R\u00b2-metric for evaluating the performance of a linear regression algorithm</li> <li>Explain the MSE metric</li> <li>Explain how regression models can be trained</li> </ul>"},{"location":"03_Regression/#video-lectures","title":"Video Lectures","text":""},{"location":"03_Regression/#31-linear-regression","title":"3.1 Linear Regression","text":""},{"location":"03_Regression/#32-regularization","title":"3.2 Regularization","text":"<p>In general we recommend Alexander Ihler\u2019s course \u201cMachine Learning and Data Mining\u201d.</p>"},{"location":"04_Data_Preparation/","title":"04 Data Preparation","text":"04 Data Preparation"},{"location":"04_Data_Preparation/#preparation","title":"Preparation","text":""},{"location":"04_Data_Preparation/#material","title":"Material","text":""},{"location":"04_Data_Preparation/#session-description","title":"Session Description","text":""},{"location":"04_Data_Preparation/#key-concepts","title":"Key Concepts","text":""},{"location":"04_Data_Preparation/#learning-objectives","title":"Learning Objectives","text":""},{"location":"04_Data_Preparation/#video-lectures","title":"Video Lectures","text":""},{"location":"05_Tree_Based_Models/","title":"05 Tree Based Models","text":"05 Tree Based Models"},{"location":"05_Tree_Based_Models/#preparation","title":"Preparation","text":""},{"location":"05_Tree_Based_Models/#material","title":"Material","text":""},{"location":"05_Tree_Based_Models/#session-description","title":"Session Description","text":""},{"location":"05_Tree_Based_Models/#key-concepts","title":"Key Concepts","text":""},{"location":"05_Tree_Based_Models/#learning-objectives","title":"Learning Objectives","text":""},{"location":"05_Tree_Based_Models/#video-lectures","title":"Video Lectures","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/","title":"06 Validation Methods and Performance Metrics","text":"06 Validation Methods and Performance Metrics"},{"location":"06_Validation_Methods_and_Performance_Metrics/#preparation","title":"Preparation","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/#material","title":"Material","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/#session-description","title":"Session Description","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/#key-concepts","title":"Key Concepts","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/#learning-objectives","title":"Learning Objectives","text":""},{"location":"06_Validation_Methods_and_Performance_Metrics/#video-lectures","title":"Video Lectures","text":""},{"location":"07_Logistic_Regression_and_Gradient_Descent/","title":"07 Logistic Regression and Gradient Descent","text":"07 Logistic Regression and Gradient Descend"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#material","title":"Material:","text":"<p>Section about \"Logistic Regression\" in Ch 4, pp. 136 - 146 </p> <p>Session material</p>"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#topics","title":"Topics","text":"<p>This lecture will cover the logistic regression algorithm </p> <p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Train a logistic regression (LR) model on a dataset.</li> <li>Understand the difference between linear regression and logistic regression.</li> <li>Understand the concept of maximum likelihood estimation.</li> <li>Explain the key ideas behind logistic regression, and implement a logistic regression classifier in python.</li> <li>Explain and use L1 and L2 regularization in the context of logistic regression, and discuss the difference between these approaches, as well as the importance of the hyperparameter C.</li> <li>Discuss advantages and disadvantages of logistic regression.</li> </ul>"},{"location":"08_Support_Vector_Machines/","title":"08 Support Vector Machines","text":"08 Support Vector Machines"},{"location":"08_Support_Vector_Machines/#preparation","title":"Preparation","text":"<p>Ch 5</p>"},{"location":"08_Support_Vector_Machines/#material","title":"Material","text":"<p>Watch the video lectures below. here is the material from the videos.</p> <p>Slides</p> <p>Code</p>"},{"location":"08_Support_Vector_Machines/#session-description","title":"Session Description","text":"<p>This lecture introduces a more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM).</p> <p>We will start with the basic concepts of SVM, including the idea of support vectors and how they influence the decision boundary. We will then discuss how to make non-linearly separable data linearly separable by adding new features. The kernel trick will be introduced as a powerful method to handle non-linear data, and we will explore how different kernels can be used in SVM.</p>"},{"location":"08_Support_Vector_Machines/#key-concepts","title":"Key Concepts","text":"<ul> <li>SVM Classification</li> <li>Support Vectors</li> <li>Decision Boundary</li> <li>Linear Separability</li> <li>Feature Transformation</li> <li>Kernel Trick</li> <li>Kernel Functions</li> <li>Non-linear Classification</li> </ul>"},{"location":"08_Support_Vector_Machines/#learning-objectives","title":"Learning Objectives","text":"<p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.</li> <li>Explain and exemplify how adding new features can make non-linearly separable data linearly separable.</li> <li>Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.</li> <li>Discuss how, when using a Gaussian kernel,\u00a0the hyperparameters C and\u00a0\u03b3 influence the decision boundaries.</li> <li>Discuss advantages and disadvantages of support vector machines.</li> <li>Discuss the concept of hyperparameters in the context of SVM, and demonstrate how to tune them to improve model performance.</li> </ul>"},{"location":"08_Support_Vector_Machines/#video-lectures","title":"Video Lectures","text":"<p>Playlist with all videos (Opens in Youtube)</p>"},{"location":"08_Support_Vector_Machines/#81-svm-introduction-to-svm-part-1","title":"8.1. SVM - Introduction to SVM (Part 1)","text":""},{"location":"08_Support_Vector_Machines/#82-svm-soft-margin-svm-part-2","title":"8.2. SVM - Soft Margin SVM (Part 2)","text":""},{"location":"08_Support_Vector_Machines/#83-svm-multiclass-classification-part-3","title":"8.3. SVM - Multiclass classification (Part 3)","text":""},{"location":"08_Support_Vector_Machines/#84-svm-non-linear-classification-part-4","title":"8.4. SVM - Non-linear classification (Part 4)","text":""},{"location":"08_Support_Vector_Machines/#85-svm-code-example-part-5","title":"8.5. SVM - Code example (Part 5)","text":""},{"location":"09_Neural_Networks/","title":"09 Neural Networks","text":"09 Introduction to Neural Networks"},{"location":"09_Neural_Networks/#preparation","title":"Preparation","text":"<p>Ch 10 + 11 until (but not including) \u201cBatch Normalization\u201d (pp. 357 \u2013 367). Feel free to ignore the mathematical definitions of the activation functions.</p> <p>For background and as reference: Ch 12</p> <p>Optional: Look at the 3Blue1Brown videos on neural networks. They are not required, but they are a good introduction to the topic and will help you understand the material better. The videos are very well made and provide a great overview of the concepts.</p>"},{"location":"09_Neural_Networks/#material","title":"Material","text":"<p>Session material</p> <p>You will need to install tensorflow and (optionally) tensorboard before this lesson. Specifically, you need tensorflow 2.0 or higher to be able to use all the functionalities in the notebooks. If you don't know whether you have these packages installed, you can open an anaconda prompt and type</p> <pre><code>pip freeze\n</code></pre> <p>to see which packages you have and which versions. You can run the following commands in the anaconda prompt to get the necessary packages:</p> <p><pre><code>pip install tensorflow  \npip install tensorboard\n</code></pre> (If you have an older version of tensorflow, you can upgrade with <code>pip install tensorflow --upgrade</code>).</p> <p>In addition to this, Tensorflow 2 requires you to install this visual studio package: https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads (if you don't have windows or would like to read more, you can do so here: https://www.tensorflow.org/install/pip).</p>"},{"location":"09_Neural_Networks/#session-description","title":"Session Description","text":"<p>This lecture covers the fundamental aspects of\u00a0neural networks.\u00a0</p>"},{"location":"09_Neural_Networks/#key-concepts","title":"Key Concepts","text":"<ul> <li>Artificial Neurons</li> <li>Perceptrons</li> <li>Network Structure</li> <li>Activation Functions</li> <li>Learning Rate</li> <li>Loss Functions</li> <li>Softmax Output</li> </ul>"},{"location":"09_Neural_Networks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Explain what is meant by artificial neurons, and how these are linked to form artificial neural networks.</li> <li>Explain what a perceptron is, and how it transforms an input vector to an output, including the importance of weights and biases.</li> <li>Discuss how to structure a neural network.</li> <li>Discuss and summarize the method of (stochastic) gradient descent and how it is used to train a neural network, including how the learning rate influences results.</li> <li>Reflect upon the problems caused by using (a) perceptrons as artificial neurons and (b) the accuracy as the metric to optimize during model training, including how these problems can be solved using activation and loss functions, respectively.</li> <li>Sketch different activation functions, including the sigmoid, tanh and ReLU functions, and discuss why the softmax activation function is generally used in the output layer.</li> <li>Implement a neural network in python using the tensorflow.keras module.</li> </ul>"},{"location":"09_Neural_Networks/#video-lectures","title":"Video Lectures","text":"<p>Playlist with all videos (Opens in Youtube) - 3Blue1Brown: Neural networks</p>"},{"location":"09_Neural_Networks/#91-neural-networks-what-is-a-neural-network-part-1","title":"9.1. Neural Networks - What is a neural network? (Part 1)","text":""},{"location":"09_Neural_Networks/#92-neural-networks-gradient-descent-how-neural-networks-learn-part-2","title":"9.2. Neural Networks - Gradient descent, how neural networks learn (Part 2)","text":""},{"location":"09_Neural_Networks/#93-neural-networks-backpropagation-part-3","title":"9.3. Neural Networks - Backpropagation (Part 3)","text":""},{"location":"09_Neural_Networks/#94-neural-networks-backpropagation-calculus-part-4","title":"9.4. Neural Networks - Backpropagation calculus (Part 4)","text":""},{"location":"09_Neural_Networks/#95-neural-networks-large-language-models-part-5","title":"9.5. Neural Networks - Large Language Models (Part 5)","text":""},{"location":"09_Neural_Networks/#96-neural-networks-transformers-part-6","title":"9.6. Neural Networks - Transformers (Part 6)","text":""},{"location":"09_Neural_Networks/#97-neural-networks-attention-in-transformers-part-7","title":"9.7. Neural Networks - Attention in transformers (Part 7)","text":""},{"location":"09_Neural_Networks/#98-neural-networks-how-might-llms-store-facts-part-8","title":"9.8. Neural Networks - How might LLMs store facts (Part 8)","text":""},{"location":"10_Dimensionality_Reduction/","title":"10 Dimensionality Reduction","text":"10 Dimensionality Reduction"},{"location":"10_Dimensionality_Reduction/#preparation","title":"Preparation","text":"<p>Ch 8</p>"},{"location":"10_Dimensionality_Reduction/#material","title":"Material","text":"<p>Session Notes X</p> <p>Session Notes Y</p> <p>Session material</p> <p>Steve Brunton has made a whole lecture series about the SVD. This is overkill but maybe check out the the Overview and the videoes about PCA.</p> <p>For a very appealing and visual explanation of SVD, you should take a look at Visual Kernel's video on the topic.</p> <p>Useful Resources on t-SNE:</p> <p>How to Use t-SNE Effectively</p> <p>openTSNE</p> <p>Documentation</p> <p>I've also added the original research papers leading up to t-SNE (not part of syllabus, just there for reference)</p> <p>If you want a really in depth introduction to t-SNE, look here</p> <p>And as always, Alexander Ihler is gold.</p> <p>If you missed the session in linear algebra, I recommend checking out some of the resources mentioned above.</p>"},{"location":"10_Dimensionality_Reduction/#session-description","title":"Session Description","text":"<p>This lecture covers unsupervised machine learning algorithms. We discuss how these can be used for dimensionality reduction. </p>"},{"location":"10_Dimensionality_Reduction/#key-concepts","title":"Key Concepts","text":"<ul> <li>Principal component analysis (PCA)</li> <li>t-distributed stochastic neighbor embedding (t-SNE)</li> </ul>"},{"location":"10_Dimensionality_Reduction/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use principle component analysis (PCA) to reduce the dimensions of your dataset</li> <li>Describe how PCA can be used for clustering analyses</li> <li>Create 2-dimensional clustering-plots in python using PCA and t-SNE</li> </ul>"},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left to access a plan of a specific session and additional resources for that session.</p> Session Date Topic 07 25 Mar 12:45 \u2013 16:05 Logistic Regression and Gradient Descent 08 1 Apr 12:45 \u2013 16:05 Support Vector Machines 09 8 Apr 12:45 \u2013 16:05 Neural Networks 10 22 Apr 12:45 \u2013 16:05 Dimensionality Reduction 11 29 Apr 12:45 \u2013 16:05 Clustering 12 To be specified later Recap"},{"location":"blog/","title":"Blog","text":""}]}