{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Machine Learning Course - Spring 2025 <p>Repository for MAL1-S25 at VIA</p> <p>Checkout the homepage!</p> </p> <p>Jump to Lesson Plan</p> <p>Jump to Portfolios and Project</p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>Bachelor level course - the course is academically challenging working on problems independently.</li> <li>Grade: 7-step scale</li> <li>Recommended prerequisites: In \"Sessions\" in the left menu, a dedicated entry is made for prerequisites.</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended to work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#literature-resources","title":"Literature &amp; Resources","text":"<p>G\u00e9ron, Aur\u00e9lien: Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow, 3<sup>rd</sup> Edition. (the second edition will also do)</p> <p>We highly recommend retrieving a copy of the book \u2013 it will also be the course book for the Deep Learning course in the Autumn.</p>"},{"location":"#software","title":"Software","text":"<p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. The easiest way to install Python and Jupyter is using Anaconda Distribution. You can choose whichever framework you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p> <p>The course will be somewhat \"Python-heavy\" and during the course, it is expected that you can solve relatively complex machine learning problems in Python (in your assignments and project). It is expected that you are able to work in Python or learn to do so relatively fast.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>There are three main areas that are important when it comes to Machine Learning</p> <ul> <li>Programming</li> <li>Linear Algebra</li> <li>Probability theory and statistics.</li> </ul> <p>We assume that you have (1) covered! Linear Algebra knowledge you can obtain either by following our course (IT-ALI1) or you can see some suggestions below under Online Resource. The same goes for item (3). Now, it is possible to master machine learning without knowing anything about linear algebra or probability theory, but some topics will most likely be easier to comprehend if you have some background knowledge about the underlying mathematical foundation.</p>"},{"location":"#online-resources","title":"Online Resources","text":"<p>There are several Python-programming tutorials on YouTube, also ones that are data science / Machine Learning oriented. I recommend Alexander Ihler\u2019s course Machine Learning and Data Mining.</p> <p>For prerequisites, we have our own course here at VIA called Applied Linear Algebra. You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2023).</p> <p>In terms of probability theory and statistics, we have our own course here at VIA called Stochastic Modelling and Processing (IT-SMP1). You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2021). </p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Introduction to Machine Learning was first offered in the spring of 2018 and has been scheduled 1-2 times per year since then. The course responsible is Richard Brooks (RIB). </p> <p>Grade Distribution 2023 (ordinary exam only)</p> Grade Count 12 13 10 7 7 15 4 7 02 5 00 6"},{"location":"07_Logistic_Regression_and_Gradient_Descent/","title":"07 Logistic Regression and Gradient Descent","text":"07 Logistic Regression and Gradient Descend"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#material","title":"Material:","text":"<p>Section about \"Logistic Regression\" in Ch 4, pp. 136 - 146 </p> <p>Session material</p>"},{"location":"07_Logistic_Regression_and_Gradient_Descent/#topics","title":"Topics","text":"<p>This lecture will cover the logistic regression algorithm </p> <p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Train a logistic regression (LR) model on a dataset.</li> <li>Understand the difference between linear regression and logistic regression.</li> <li>Understand the concept of maximum likelihood estimation.</li> <li>Explain the key ideas behind logistic regression, and implement a logistic regression classifier in python.</li> <li>Explain and use L1 and L2 regularization in the context of logistic regression, and discuss the difference between these approaches, as well as the importance of the hyperparameter C.</li> <li>Discuss advantages and disadvantages of logistic regression.</li> </ul>"},{"location":"08_Support_Vector_Machines/","title":"08 Support Vector Machines","text":"08 Support Vector Machines"},{"location":"08_Support_Vector_Machines/#preparation","title":"Preparation","text":"<p>Ch 5</p>"},{"location":"08_Support_Vector_Machines/#material","title":"Material","text":"<p>Watch the video lectures below. here is the material from the videos.</p> <p>Slides</p> <p>Code</p>"},{"location":"08_Support_Vector_Machines/#session-description","title":"Session Description","text":"<p>This lecture introduces a more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM).</p> <p>We will start with the basic concepts of SVM, including the idea of support vectors and how they influence the decision boundary. We will then discuss how to make non-linearly separable data linearly separable by adding new features. The kernel trick will be introduced as a powerful method to handle non-linear data, and we will explore how different kernels can be used in SVM.</p>"},{"location":"08_Support_Vector_Machines/#key-concepts","title":"Key Concepts","text":"<ul> <li>SVM Classification</li> <li>Support Vectors</li> <li>Decision Boundary</li> <li>Linear Separability</li> <li>Feature Transformation</li> <li>Kernel Trick</li> <li>Kernel Functions</li> <li>Non-linear Classification</li> </ul>"},{"location":"08_Support_Vector_Machines/#learning-objectives","title":"Learning Objectives","text":"<p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.</li> <li>Explain and exemplify how adding new features can make non-linearly separable data linearly separable.</li> <li>Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.</li> <li>Discuss how, when using a Gaussian kernel,\u00a0the hyperparameters C and\u00a0\u03b3 influence the decision boundaries.</li> <li>Discuss advantages and disadvantages of support vector machines.</li> <li>Discuss the concept of hyperparameters in the context of SVM, and demonstrate how to tune them to improve model performance.</li> </ul>"},{"location":"08_Support_Vector_Machines/#video-lectures","title":"Video Lectures","text":"<p>Playlist with all videos (Opens in Youtube)</p>"},{"location":"08_Support_Vector_Machines/#81-svm-introduction-to-svm-part-1","title":"8.1. SVM - Introduction to SVM (Part 1)","text":""},{"location":"08_Support_Vector_Machines/#82-svm-soft-margin-svm-part-2","title":"8.2. SVM - Soft Margin SVM (Part 2)","text":""},{"location":"08_Support_Vector_Machines/#83-svm-multiclass-classification-part-3","title":"8.3. SVM - Multiclass classification (Part 3)","text":""},{"location":"08_Support_Vector_Machines/#84-svm-non-linear-classification-part-4","title":"8.4. SVM - Non-linear classification (Part 4)","text":""},{"location":"08_Support_Vector_Machines/#85-svm-code-example-part-5","title":"8.5. SVM - Code example (Part 5)","text":""},{"location":"09_Neural_Networks/","title":"09 Neural Networks","text":"09 Introduction to Neural Networks"},{"location":"09_Neural_Networks/#preparation","title":"Preparation","text":"<p>Ch 10 + 11 until (but not including) \u201cBatch Normalization\u201d (pp. 357 \u2013 367). Feel free to ignore the mathematical definitions of the activation functions.</p> <p>For background and as reference: Ch 12</p> <p>Optional: Look at the 3Blue1Brown videos on neural networks. They are not required, but they are a good introduction to the topic and will help you understand the material better. The videos are very well made and provide a great overview of the concepts.</p>"},{"location":"09_Neural_Networks/#material","title":"Material","text":"<p>Session material: In this folder</p> <p>You will need to install tensorflow and (optionally) tensorboard before this lesson. Specifically, you need tensorflow 2.0 or higher to be able to use all the functionalities in the notebooks. If you don't know whether you have these packages installed, you can open an anaconda prompt and type</p> <pre><code>pip freeze\n</code></pre> <p>to see which packages you have and which versions. You can run the following commands in the anaconda prompt to get the necessary packages:</p> <p><pre><code>pip install tensorflow  \npip install tensorboard\n</code></pre> (If you have an older version of tensorflow, you can upgrade with <code>pip install tensorflow --upgrade</code>).</p> <p>In addition to this, Tensorflow 2 requires you to install this visual studio package: https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads (if you don't have windows or would like to read more, you can do so here: https://www.tensorflow.org/install/pip).</p>"},{"location":"09_Neural_Networks/#session-description","title":"Session Description","text":"<p>This lecture covers the fundamental aspects of\u00a0neural networks.\u00a0</p>"},{"location":"09_Neural_Networks/#key-concepts","title":"Key Concepts","text":"<ul> <li>Artificial Neurons</li> <li>Perceptrons</li> <li>Network Structure</li> <li>Activation Functions</li> <li>Learning Rate</li> <li>Loss Functions</li> <li>Softmax Output</li> </ul>"},{"location":"09_Neural_Networks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Explain what is meant by artificial neurons, and how these are linked to form artificial neural networks.</li> <li>Explain what a perceptron is, and how it transforms an input vector to an output, including the importance of weights and biases.</li> <li>Discuss how to structure a neural network.</li> <li>Discuss and summarize the method of (stochastic) gradient descent and how it is used to train a neural network, including how the learning rate influences results.</li> <li>Reflect upon the problems caused by using (a) perceptrons as artificial neurons and (b) the accuracy as the metric to optimize during model training, including how these problems can be solved using activation and loss functions, respectively.</li> <li>Sketch different activation functions, including the sigmoid, tanh and ReLU functions, and discuss why the softmax activation function is generally used in the output layer.</li> <li>Implement a neural network in python using the tensorflow.keras module.</li> </ul>"},{"location":"09_Neural_Networks/#video-lectures","title":"Video Lectures","text":"<p>Playlist with all videos (Opens in Youtube) - 3Blue1Brown: Neural networks</p>"},{"location":"09_Neural_Networks/#91-neural-networks-what-is-a-neural-network-part-1","title":"9.1. Neural Networks - What is a neural network? (Part 1)","text":""},{"location":"09_Neural_Networks/#92-neural-networks-gradient-descent-how-neural-networks-learn-part-2","title":"9.2. Neural Networks - Gradient descent, how neural networks learn (Part 2)","text":""},{"location":"09_Neural_Networks/#93-neural-networks-backpropagation-part-3","title":"9.3. Neural Networks - Backpropagation (Part 3)","text":""},{"location":"09_Neural_Networks/#94-neural-networks-backpropagation-calculus-part-4","title":"9.4. Neural Networks - Backpropagation calculus (Part 4)","text":""},{"location":"09_Neural_Networks/#95-neural-networks-large-language-models-part-5","title":"9.5. Neural Networks - Large Language Models (Part 5)","text":""},{"location":"09_Neural_Networks/#96-neural-networks-transformers-part-6","title":"9.6. Neural Networks - Transformers (Part 6)","text":""},{"location":"09_Neural_Networks/#97-neural-networks-attention-in-transformers-part-7","title":"9.7. Neural Networks - Attention in transformers (Part 7)","text":""},{"location":"09_Neural_Networks/#98-neural-networks-how-might-llms-store-facts-part-8","title":"9.8. Neural Networks - How might LLMs store facts (Part 8)","text":""},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left to access a plan of a specific session and additional resources for that session.</p> Session Date Topic 07 25 Mar 12:45 \u2013 16:05 Logistic Regression and Gradient Descent 08 1 Apr 12:45 \u2013 16:05 Support Vector Machines 09 8 Apr 12:45 \u2013 16:05 Neural Networks 10 22 Apr 12:45 \u2013 16:05 Dimensionality Reduction 11 29 Apr 12:45 \u2013 16:05 Clustering 12 To be specified later Recap"},{"location":"blog/","title":"Blog","text":""}]}