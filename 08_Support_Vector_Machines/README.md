<h1 align="center">08 Support Vector Machines</h1>

## Material:

[Session notes (Monday) - from page 5](https://drive.google.com/file/d/1b8-u6oY8FHurOY4OYRYF6BbIRmhz9XNl/view?usp=sharing)

[Session notes (Tuesday) - from page 5](https://drive.google.com/file/d/1J-_dAEpwynMV2PzUnUJGHhvDFE4eBim9/view?usp=sharing)

<!-- NOTE: for both sources I checked and added that SVM is from page 5 -->

## Topics

This lecture will delve into more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM).

After attending this lecture and reading the corresponding part of the book, I expect you to be able to:

- Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.
- Explain and exemplify how adding new features can make non-linearly separable data linearly separable.
- Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.
- Discuss how, when using a Gaussian kernel, the hyperparameters C and γ influence the decision boundaries.
- Discuss advantages and disadvantages of support vector machines.
- Discuss the concept of hyperparameters in the context of SVM, and demonstrate how to tune them to improve model performance.
<!-- NOTE: Should be chcked, I edited it to only keep SVM -->